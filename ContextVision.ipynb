{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devaru-ai/ContextVision/blob/main/ContextVision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required packages"
      ],
      "metadata": {
        "id": "p3PyUCDLn8Br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers faiss-cpu datasets gradio\n"
      ],
      "metadata": {
        "id": "2hBP6wCnk-GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import all necessary libraries\n"
      ],
      "metadata": {
        "id": "z9Q0Gmy2nmvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import faiss\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import random\n"
      ],
      "metadata": {
        "id": "mbwiWTj9k4Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset\n",
        "\n",
        "We use the Flickr8k dataset, which contains 8,000 images with five captions each. For evaluation, we select a subset of queries and their corresponding ground truth images.\n"
      ],
      "metadata": {
        "id": "qpzguLADK4br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"jxie/flickr8k\")['train']\n"
      ],
      "metadata": {
        "id": "KNaNgyWTk7Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    captions, images = zip(*batch)\n",
        "    return list(captions), list(images)\n"
      ],
      "metadata": {
        "id": "UsFvn4HVlYJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Flickr8kPairDataset(Dataset):\n",
        "    def __init__(self, hf_dataset):\n",
        "        self.data = hf_dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        caption = item[f'caption_{np.random.randint(0,5)}']  # Random caption for each image\n",
        "        image = item['image']\n",
        "        return caption, image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "pair_dataset = Flickr8kPairDataset(dataset)\n",
        "pair_loader = DataLoader(pair_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n"
      ],
      "metadata": {
        "id": "0X-nxOuPlMw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load CLIP model and processor (ViT-B/32) and move to GPU if available\n"
      ],
      "metadata": {
        "id": "q-ZeDlwlLLfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model.to(device)\n"
      ],
      "metadata": {
        "id": "Rj_Drmz8lZkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define symmetric contrastive loss for CLIP (aligns image and text embeddings)\n"
      ],
      "metadata": {
        "id": "n08bag68LidY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
        "    image_embeds = F.normalize(image_embeds, p=2, dim=1)\n",
        "    text_embeds = F.normalize(text_embeds, p=2, dim=1)\n",
        "    logits = image_embeds @ text_embeds.t() / temperature  # [N, N]\n",
        "    labels = torch.arange(len(image_embeds)).to(image_embeds.device)\n",
        "    loss_i2t = F.cross_entropy(logits, labels)\n",
        "    loss_t2i = F.cross_entropy(logits.t(), labels)\n",
        "    return (loss_i2t + loss_t2i) / 2\n"
      ],
      "metadata": {
        "id": "gBk_J3Fdlbg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune CLIP on (image, caption) pairs using contrastive loss and the Adam optimizer\n"
      ],
      "metadata": {
        "id": "pOxEt2vFLw_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(clip_model.parameters(), lr=1e-5)\n",
        "num_epochs = 6\n",
        "\n",
        "clip_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for captions, images in pair_loader:\n",
        "      text_inputs = clip_processor(text=captions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "      text_emb = clip_model.get_text_features(**text_inputs)\n",
        "      img_inputs = clip_processor(images=images, return_tensors=\"pt\").to(device)\n",
        "      img_emb = clip_model.get_image_features(**img_inputs)\n",
        "      loss = clip_contrastive_loss(img_emb, text_emb)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} completed. Last batch loss: {loss.item():.4f}\")\n",
        "clip_model.eval()\n"
      ],
      "metadata": {
        "id": "Mlk9Jsn-leHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract image embeddings and build a FAISS index for fast similarity search\n"
      ],
      "metadata": {
        "id": "QFuR8p6CL8vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "image_embeddings = []\n",
        "image_indices = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        batch_imgs = [dataset[j]['image'] for j in range(i, min(i + batch_size, len(dataset)))]\n",
        "        inputs = clip_processor(images=batch_imgs, return_tensors=\"pt\", padding=True).to(device)\n",
        "        emb = clip_model.get_image_features(**inputs)\n",
        "        emb = emb.cpu().numpy()\n",
        "        image_embeddings.append(emb)\n",
        "        for j in range(i, min(i + batch_size, len(dataset))):\n",
        "            image_indices.append(j)\n",
        "\n",
        "image_embeddings = np.vstack(image_embeddings)\n",
        "np.save(\"image_embeddings.npy\", image_embeddings)\n",
        "np.save(\"image_indices.npy\", np.array(image_indices))\n"
      ],
      "metadata": {
        "id": "Jb6aevDwlijD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = image_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "index.add(image_embeddings)\n",
        "faiss.write_index(index, \"image_index.faiss\")\n"
      ],
      "metadata": {
        "id": "eIU03TxjlkZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Image Retrieval Functions"
      ],
      "metadata": {
        "id": "DL626MEaMkjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_images_by_text_baseline(text_query, top_k=5, return_images=False):\n",
        "    text_inputs = clip_processor(text=[text_query], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        text_emb = clip_model.get_text_features(**text_inputs).cpu().numpy()\n",
        "    distances, indices = index.search(text_emb, k=top_k)\n",
        "    if return_images:\n",
        "        return [dataset[int(i)]['image'] for i in indices[0]]\n",
        "    else:\n",
        "        return [int(i) for i in indices[0]]\n"
      ],
      "metadata": {
        "id": "4SQT_2ORlk8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_images_by_image(query_image, top_k=5, return_images=False):\n",
        "    img_inputs = clip_processor(images=[query_image], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        query_emb = clip_model.get_image_features(**img_inputs).cpu().numpy()\n",
        "    distances, indices = index.search(query_emb, k=top_k)\n",
        "    if return_images:\n",
        "        return [dataset[int(i)]['image'] for i in indices[0]]\n",
        "    else:\n",
        "        return [int(i) for i in indices[0]]\n"
      ],
      "metadata": {
        "id": "iz5VFKbblpa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build an interactive Gradio app for text and image-based image search\n"
      ],
      "metadata": {
        "id": "lOAqCIdsMs27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# ContextVision: Multimodal Image Search\")\n",
        "\n",
        "    with gr.Tab(\"Text to Image Search\"):\n",
        "        text_input = gr.Textbox(label=\"Enter text to find similar images\")\n",
        "        text_output = gr.Gallery(label=\"Retrieved Images\")\n",
        "        text_input.change(\n",
        "            fn=lambda q: search_images_by_text_baseline(q, top_k=5, return_images=True),\n",
        "            inputs=text_input,\n",
        "            outputs=text_output\n",
        "        )\n",
        "\n",
        "\n",
        "    with gr.Tab(\"Image to Image Search\"):\n",
        "        image_input = gr.Image(type=\"pil\", label=\"Query Image\")\n",
        "        image_output = gr.Gallery(label=\"Retrieved Images\")\n",
        "        image_input.change(\n",
        "            fn=lambda img: search_images_by_image(img, top_k=5, return_images=True),\n",
        "            inputs=image_input,\n",
        "            outputs=image_output\n",
        "        )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "5cUB4_xAsOIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metrics\n"
      ],
      "metadata": {
        "id": "5I1uZHLqZ6Fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def precision_at_k(retrieved, relevant, k):\n",
        "    retrieved_k = retrieved[:k]\n",
        "    return len(set(retrieved_k) & set(relevant)) / k\n",
        "\n",
        "def recall_at_k(retrieved, relevant, k):\n",
        "    retrieved_k = retrieved[:k]\n",
        "    return len(set(retrieved_k) & set(relevant)) / len(relevant) if len(relevant) > 0 else 0.0\n",
        "\n",
        "def reciprocal_rank(retrieved, relevant):\n",
        "    for rank, idx in enumerate(retrieved, start=1):\n",
        "        if idx in relevant:\n",
        "            return 1.0 / rank\n",
        "    return 0.0\n",
        "\n",
        "def dcg(relevances):\n",
        "    return np.sum(relevances / np.log2(np.arange(2, len(relevances) + 2)))\n",
        "\n",
        "def ndcg(retrieved, relevant, k):\n",
        "    relevances = [1 if idx in relevant else 0 for idx in retrieved[:k]]\n",
        "    idcg = dcg(sorted(relevances, reverse=True))\n",
        "    return dcg(relevances) / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# --- Step 1: Prepare queries and ground truths ---\n",
        "queries = []\n",
        "ground_truths = []\n",
        "for i in range(100):\n",
        "    item = dataset[i]\n",
        "    query = item['caption_1']\n",
        "    queries.append(query)\n",
        "    ground_truths.append([i])\n",
        "\n",
        "# --- Step 2: Run retrieval/search function for each query ---\n",
        "retrieved_results = []\n",
        "for query in queries:\n",
        "    retrieved_indices = search_images_by_text_baseline(query, top_k=5)\n",
        "    retrieved_results.append(retrieved_indices)\n",
        "\n",
        "# --- Step 3: Compute metrics ---\n",
        "k = 5\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "mrr_scores = []\n",
        "ndcg_scores = []\n",
        "\n",
        "for relevant, retrieved in zip(ground_truths, retrieved_results):\n",
        "    precision_scores.append(precision_at_k(retrieved, relevant, k))\n",
        "    recall_scores.append(recall_at_k(retrieved, relevant, k))\n",
        "    mrr_scores.append(reciprocal_rank(retrieved, relevant))\n",
        "    ndcg_scores.append(ndcg(retrieved, relevant, k))\n",
        "\n",
        "mean_precision = np.mean(precision_scores)\n",
        "mean_recall = np.mean(recall_scores)\n",
        "mean_mrr = np.mean(mrr_scores)\n",
        "mean_ndcg = np.mean(ndcg_scores)\n",
        "\n",
        "print(f\"Precision@{k}: {mean_precision:.3f}\")\n",
        "print(f\"Recall@{k}: {mean_recall:.3f}\")\n",
        "print(f\"MRR: {mean_mrr:.3f}\")\n",
        "print(f\"nDCG@{k}: {mean_ndcg:.3f}\")\n"
      ],
      "metadata": {
        "id": "Rs42RL_lPWmp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}